# -*- coding: utf-8 -*-
"""StreamlitApp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mHPzEcyDaf1vv2eQTBBuLkK-mCQHpzgP
"""

"""
üéØ NLP Text Classification with Translation - Streamlit Version
Analyzes transcripts for sentiment, category, and subcategory classification
Includes automatic language detection and translation
Outputs results in CSV and Parquet formats
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import time
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import io

# NLP Libraries
from textblob import TextBlob
from afinn import Afinn
from langdetect import detect, DetectorFactory, LangDetectException
from googletrans import Translator

warnings.filterwarnings("ignore")
DetectorFactory.seed = 0

# Initialize translator
translator = Translator()

# ============================================================
# üîß NLTK Data Setup
# ============================================================
import nltk
import ssl

# Handle SSL certificate verification for NLTK downloads
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

@st.cache_resource
def download_nltk_data():
    """Download required NLTK data on first run (cached)"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    try:
        nltk.data.find('corpora/brown')
    except LookupError:
        nltk.download('brown', quiet=True)
    return True

# Download NLTK data (runs once, then cached)
download_nltk_data()

# ============================================================
# üìä Configuration
# ============================================================
class Config:
    """Application configuration settings"""
    NUM_THREADS = min(4, os.cpu_count() or 4)  # Conservative for Streamlit
    MAX_FILE_SIZE_MB = 100
    MAX_ROWS = 50000
    SENTIMENT_WEIGHTS = {'textblob': 0.6, 'afinn': 0.4}
    SENTIMENT_THRESHOLDS = {
        'very_negative': -0.75,
        'negative': -0.25,
        'positive': 0.25,
        'very_positive': 0.75
    }
    MAX_PREVIEW_ROWS = 100
    OUTPUT_DIR = "nlp_results"
    MIN_TEXT_LENGTH = 3
    CHUNK_SIZE = 1000
    ENABLE_TRANSLATION = True
    TRANSLATION_BATCH_DELAY = 0.5


# ============================================================
# üîë Category and Subcategory Keywords
# ============================================================
TOPIC_KEYWORDS = {
    "login issue": [
        "login", "log in", "sign in", "sign-in", "signin", "sign out", "sign-out", "signout",
        "password", "forgot password", "reset password", "authentication",
        "verify account", "verification code", "2fa", "two-factor", "two factor",
        "unable to access account", "can't log in", "cannot login"
    ],
    "account issue": [
        "account", "profile", "username", "display name",
        "linked account", "merge account", "multiple accounts",
        "email change", "update details", "account disabled",
        "account locked", "deactivate account", "delete account"
    ],
    "playback issue": [
        "playback", "stream", "music not playing", "song not playing",
        "track skipped", "buffering", "lag", "pause", "stuck",
        "stops suddenly", "won't play", "audio issue", "no sound",
        "silence", "volume problem", "audio quality"
    ],
    "device issue": [
        "bluetooth", "speaker", "carplay", "android auto", "smart tv",
        "echo", "alexa", "chromecast", "airplay", "headphones",
        "device not showing", "device disconnected", "connection issue"
    ],
    "content restriction": [
        "song not available", "track unavailable", "region restriction",
        "country restriction", "not licensed", "greyed out", "removed song",
        "can't find song", "missing track"
    ],
    "ad issue": [
        "ads", "advertisement", "too many ads", "ad volume",
        "ad playing", "premium ads", "commercials", "ad frequency"
    ],
    "recommendation issue": [
        "recommendations", "discover weekly", "radio", "algorithm",
        "curated", "autoplay", "song suggestions", "not relevant",
        "bad recommendations"
    ],
    "ui issue": [
        "interface", "layout", "design", "dark mode", "theme",
        "buttons not working", "search not working", "filter not working",
        "navigation", "menu"
    ],
    "general feedback": [
        "suggestion", "feedback", "recommend", "love spotify",
        "like app", "app improvement", "feature request", "enhancement"
    ],
    "network failure": [
        "network", "connectivity", "internet", "server",
        "connection failed", "offline", "not connecting",
        "spotify down", "timeout", "dns", "proxy", "vpn"
    ],
    "app crash": [
        "crash", "crashed", "app closed", "stopped working", "freeze",
        "freezing", "hang", "bug", "error message", "glitch",
        "unresponsive", "not responding"
    ],
    "performance issue": [
        "slow", "lag", "delay", "performance", "loading", "slow loading",
        "takes forever", "laggy"
    ],
    "data sync issue": [
        "sync", "not syncing", "listening history", "recently played",
        "activity feed", "spotify connect", "data lost", "missing data",
        "playlist not syncing"
    ],
    "subscription issue": [
        "subscription", "plan", "premium", "cancel", "renew",
        "billing", "charged", "payment", "refund", "invoice",
        "upgrade", "downgrade", "free trial", "family plan",
        "student plan", "gift card", "promo code", "spotify wrapped",
        "card", "payment failed"
    ],
}

SUBCATEGORY_KEYWORDS = {
    "subscription issue": {
        "payment": ["refund", "charged", "billing", "invoice", "payment", "payment failed", "card declined"],
        "cancel": ["cancel", "unsubscribe", "stop subscription", "end subscription"],
        "upgrade": ["upgrade", "family plan", "student plan", "premium", "switch plan"],
    },
    "account issue": {
        "login": ["login", "password", "signin", "sign in", "authentication"],
        "profile": ["profile", "email", "username", "display name", "account settings"],
    },
    "device issue": {
        "mobile": ["phone", "android", "iphone", "ios", "mobile app"],
        "car": ["carplay", "android auto", "car", "vehicle"],
        "smart_device": ["alexa", "echo", "chromecast", "smart tv", "airplay"],
    },
}

# Pre-compile regex patterns
CONSUMER_PATTERN_PRIMARY = re.compile(
    r"(?i)Consumer:\s*(.*?)(?=\s*\|\s*\d{4}-\d{2}-\d{2}|$|\s*\|\s*Agent:)",
    re.IGNORECASE
)
CONSUMER_PATTERN_FALLBACK = re.compile(
    r"(?i)Consumer:\s*(.*?)(?=\||$)",
    re.IGNORECASE
)


# ============================================================
# üõ†Ô∏è Helper Functions
# ============================================================

def detect_language(text: str) -> str:
    """Detect the language of the text."""
    if not text or len(text.strip()) < Config.MIN_TEXT_LENGTH:
        return 'unknown'
    try:
        return detect(text)
    except LangDetectException:
        return 'unknown'
    except Exception as e:
        st.warning(f"Language detection error: {e}")
        return 'unknown'


def translate_to_english(text: str, source_lang: str) -> Tuple[str, bool]:
    """Translate text to English if it's in a foreign language."""
    if not text or source_lang == 'en' or source_lang == 'unknown':
        return text, False

    try:
        time.sleep(Config.TRANSLATION_BATCH_DELAY)
        translated = translator.translate(text, src=source_lang, dest='en')
        return translated.text, True
    except Exception as e:
        st.warning(f"Translation error ({source_lang} -> en): {e}")
        return text, False


def extract_consumer_text(transcript: str) -> str:
    """Extract consumer text from transcript."""
    if not isinstance(transcript, str):
        return ""

    parts = CONSUMER_PATTERN_PRIMARY.findall(transcript + " ")

    if not parts:
        parts = CONSUMER_PATTERN_FALLBACK.findall(transcript + "|")

    return " ".join(p.strip() for p in parts if p.strip())


def hybrid_sentiment(text: str, af: Afinn) -> str:
    """Perform hybrid sentiment analysis using TextBlob and AFINN."""
    if not text or len(text.strip()) < Config.MIN_TEXT_LENGTH:
        return ""

    if len(text.split()) < Config.MIN_TEXT_LENGTH:
        return "neutral"

    tb_score = TextBlob(text).sentiment.polarity
    af_score = af.score(text) / 5.0

    score = (Config.SENTIMENT_WEIGHTS['textblob'] * tb_score +
             Config.SENTIMENT_WEIGHTS['afinn'] * af_score)

    thresholds = Config.SENTIMENT_THRESHOLDS
    if score <= thresholds['very_negative']:
        return "very negative"
    elif score <= thresholds['negative']:
        return "negative"
    elif score >= thresholds['very_positive']:
        return "very positive"
    elif score >= thresholds['positive']:
        return "positive"
    else:
        return "neutral"


def predict_category(text: str) -> Tuple[str, int]:
    """Predict category with confidence score."""
    text_lower = text.lower()
    best_match, best_score = "", 0

    for category, keywords in TOPIC_KEYWORDS.items():
        score = 0
        for keyword in keywords:
            if len(keyword.split()) == 1 and len(keyword) <= 3:
                pattern = r'\b' + re.escape(keyword) + r'\b'
                if re.search(pattern, text_lower):
                    score += 1
            else:
                if keyword in text_lower:
                    score += 1.5 if ' ' in keyword else 1

        if score > best_score:
            best_score = score
            best_match = category

    return (best_match if best_score > 0 else "", int(best_score))


def predict_subcategory(category: str, text: str) -> Tuple[str, int]:
    """Predict subcategory based on category and text."""
    if not category or category not in SUBCATEGORY_KEYWORDS:
        return ("", 0)

    text_lower = text.lower()
    best_match, best_score = "", 0

    for subcategory, keywords in SUBCATEGORY_KEYWORDS[category].items():
        score = 0
        for keyword in keywords:
            if len(keyword.split()) == 1 and len(keyword) <= 3:
                pattern = r'\b' + re.escape(keyword) + r'\b'
                if re.search(pattern, text_lower):
                    score += 1
            else:
                if keyword in text_lower:
                    score += 1.5 if ' ' in keyword else 1

        if score > best_score:
            best_score = score
            best_match = subcategory

    return (best_match, int(best_score))


def apply_rules(text: str, preds: Dict) -> Dict:
    """Apply rule-based overrides for special cases."""
    text_lower = text.lower()

    if any(k in text_lower for k in ["refund", "charged", "billing", "payment failed"]):
        preds["category"] = "subscription issue"
        preds["subcategory"] = "payment"
        if "refund" in text_lower or "charged" in text_lower:
            preds["sentiment"] = "negative"

    elif "cancel" in text_lower and "subscription" in text_lower:
        preds["category"] = "subscription issue"
        preds["subcategory"] = "cancel"

    return preds


# ============================================================
# üîÑ Core Processing Function
# ============================================================

def process_row(row: Dict, af: Afinn) -> Dict:
    """Process a single transcript row and classify it."""
    conversation_id = row.get("Conversation Id", "")
    transcript = str(row.get("transcripts", ""))
    consumer_text = extract_consumer_text(transcript)

    if not consumer_text.strip():
        return {
            "Conversation Id": conversation_id,
            "Consumer_Text": consumer_text,
            "Translated_Text": "",
            "Category": "",
            "Subcategory": "",
            "Sentiment": "",
        }

    detected_lang = detect_language(consumer_text)
    translated_text = ""
    text_for_analysis = consumer_text

    if Config.ENABLE_TRANSLATION and detected_lang != 'en' and detected_lang != 'unknown':
        translated_text, was_translated = translate_to_english(consumer_text, detected_lang)
        if was_translated:
            text_for_analysis = translated_text

    if detected_lang == 'unknown' or len(text_for_analysis.split()) < Config.MIN_TEXT_LENGTH:
        return {
            "Conversation Id": conversation_id,
            "Consumer_Text": consumer_text,
            "Translated_Text": translated_text,
            "Category": "",
            "Subcategory": "",
            "Sentiment": "",
        }

    category, cat_confidence = predict_category(text_for_analysis)
    subcategory, subcat_confidence = predict_subcategory(category, text_for_analysis)
    sentiment = hybrid_sentiment(text_for_analysis, af)

    preds = {
        "category": category,
        "subcategory": subcategory,
        "sentiment": sentiment,
        "category_confidence": cat_confidence,
        "subcategory_confidence": subcat_confidence,
    }

    preds = apply_rules(text_for_analysis, preds)

    return {
        "Conversation Id": conversation_id,
        "Consumer_Text": consumer_text,
        "Translated_Text": translated_text,
        "Category": preds["category"],
        "Subcategory": preds["subcategory"],
        "Sentiment": preds["sentiment"],
    }


# ============================================================
# üìä Data Validation
# ============================================================

def validate_input_dataframe(df: pd.DataFrame) -> None:
    """Validate input dataframe structure and content."""
    required_cols = ["Conversation Id", "transcripts"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"‚ùå Missing required columns: {', '.join(missing_cols)}")

    if len(df) == 0:
        raise ValueError("‚ùå Input file is empty")

    if len(df) > Config.MAX_ROWS:
        raise ValueError(f"‚ùå Too many rows: {len(df):,} (max: {Config.MAX_ROWS:,})")

    null_count = df["transcripts"].isnull().sum()
    if null_count == len(df):
        raise ValueError("‚ùå All transcript values are empty")

    if null_count > len(df) * 0.5:
        st.warning(f"‚ö†Ô∏è Warning: {null_count:,}/{len(df):,} ({null_count/len(df)*100:.1f}%) transcripts are empty")


# ============================================================
# üöÄ Main Pipeline Function
# ============================================================

def run_pipeline(df: pd.DataFrame, progress_bar=None, status_text=None) -> pd.DataFrame:
    """Run the NLP classification pipeline with parallel processing."""
    start_time = time.time()
    af = Afinn()

    rows = df.to_dict("records")
    total_rows = len(rows)

    results = []
    processed = 0

    with ThreadPoolExecutor(max_workers=Config.NUM_THREADS) as executor:
        future_to_row = {executor.submit(process_row, row, af): row for row in rows}

        for future in as_completed(future_to_row):
            results.append(future.result())
            processed += 1

            if progress_bar and processed % 10 == 0:
                progress = processed / total_rows
                progress_bar.progress(progress)
                if status_text:
                    status_text.text(f"Processing: {processed:,}/{total_rows:,} rows ({progress*100:.1f}%)")

    if progress_bar:
        progress_bar.progress(1.0)
    if status_text:
        elapsed = time.time() - start_time
        status_text.text(f"‚úÖ Completed! Processed {total_rows:,} rows in {elapsed:.2f}s ({total_rows/elapsed:.1f} rows/sec)")

    out_df = pd.DataFrame(results)

    return out_df


# ============================================================
# üé® Streamlit UI
# ============================================================

def main():
    """Main Streamlit application"""

    # Page configuration
    st.set_page_config(
        page_title="NLP Text Classification with Translation",
        page_icon="üéØ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Custom CSS
    st.markdown("""
        <style>
        .main-header {
            font-size: 2.5rem;
            font-weight: bold;
            color: #1f77b4;
            text-align: center;
            margin-bottom: 1rem;
        }
        .metric-card {
            background-color: #f0f2f6;
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 0.5rem 0;
        }
        .stDownloadButton>button {
            width: 100%;
        }
        </style>
    """, unsafe_allow_html=True)

    # Header
    st.markdown('<p class="main-header">üéØ NLP Text Classification with Translation</p>', unsafe_allow_html=True)
    st.markdown("**Analyze customer service transcripts with automatic language detection & translation**")
    st.markdown("---")

    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")

        # File upload
        st.subheader("üìÅ Upload Dataset")
        uploaded_file = st.file_uploader(
            "Choose a CSV or Excel file",
            type=["csv", "xlsx"],
            help="File must contain 'Conversation Id' and 'transcripts' columns"
        )

        st.markdown("---")

        # Settings
        st.subheader("üîß Settings")

        enable_translation = st.checkbox(
            "Enable Translation",
            value=Config.ENABLE_TRANSLATION,
            help="Automatically translate non-English text to English"
        )
        Config.ENABLE_TRANSLATION = enable_translation

        num_threads = st.slider(
            "Number of threads",
            min_value=1,
            max_value=os.cpu_count() or 4,
            value=Config.NUM_THREADS,
            help="More threads = faster processing"
        )
        Config.NUM_THREADS = num_threads

        if enable_translation:
            translation_delay = st.slider(
                "Translation delay (seconds)",
                min_value=0.1,
                max_value=2.0,
                value=Config.TRANSLATION_BATCH_DELAY,
                step=0.1,
                help="Delay between translations to avoid rate limits"
            )
            Config.TRANSLATION_BATCH_DELAY = translation_delay

        preview_rows = st.slider(
            "Preview rows",
            min_value=10,
            max_value=500,
            value=Config.MAX_PREVIEW_ROWS,
            step=10,
            help="Number of rows to display in preview"
        )

        st.markdown("---")

        # Info
        st.subheader("‚ÑπÔ∏è About")
        st.info(
            "This app classifies customer service transcripts by:\n"
            "- **Category**: Main issue type\n"
            "- **Subcategory**: Specific issue\n"
            "- **Sentiment**: Customer emotion\n"
            "- **Translation**: Auto-translates foreign languages\n"
            "\n**Output**: CSV & Parquet formats"
        )

        # System info
        with st.expander("üñ•Ô∏è System Info"):
            st.text(f"Python: {os.sys.version.split()[0]}")
            st.text(f"CPU Cores: {os.cpu_count()}")
            st.text(f"Max File Size: {Config.MAX_FILE_SIZE_MB} MB")
            st.text(f"Max Rows: {Config.MAX_ROWS:,}")
            st.text(f"Translation: {'Enabled' if Config.ENABLE_TRANSLATION else 'Disabled'}")

    # Main content
    if uploaded_file is None:
        # Welcome screen
        st.info("üëà **Get Started:** Upload a CSV or Excel file using the sidebar")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("### üìù Step 1")
            st.write("Upload your dataset with conversation transcripts")

        with col2:
            st.markdown("### üöÄ Step 2")
            st.write("Click 'Run Analysis' to process the data")

        with col3:
            st.markdown("### üì• Step 3")
            st.write("Download results in CSV or Parquet format")

        st.markdown("---")

        # Sample data format
        st.subheader("üìã Required Data Format")
        sample_df = pd.DataFrame({
            "Conversation Id": ["CONV_001", "CONV_002", "CONV_003"],
            "transcripts": [
                "Consumer: I can't login to my account | Agent: Let me help you",
                "Consumer: No puedo iniciar sesi√≥n | Agent: Te ayudo",
                "Consumer: Songs are not playing | Agent: I'll check that"
            ]
        })
        st.dataframe(sample_df, use_container_width=True)

        st.markdown("---")

        # Output format
        st.subheader("üìä Output Format (6 Columns)")
        output_example = pd.DataFrame({
            "Conversation Id": ["CONV_001", "CONV_002"],
            "Consumer_Text": ["I can't login to my account", "No puedo iniciar sesi√≥n"],
            "Translated_Text": ["", "I cannot log in"],
            "Category": ["login issue", "login issue"],
            "Subcategory": ["login", "login"],
            "Sentiment": ["negative", "negative"]
        })
        st.dataframe(output_example, use_container_width=True)

        # Categories info
        with st.expander("üìö Supported Categories (14 Total)"):
            cols = st.columns(2)
            categories = list(TOPIC_KEYWORDS.keys())
            mid = len(categories) // 2

            with cols[0]:
                for cat in categories[:mid]:
                    st.write(f"‚Ä¢ {cat}")

            with cols[1]:
                for cat in categories[mid:]:
                    st.write(f"‚Ä¢ {cat}")

    else:
        # File info
        file_size_mb = uploaded_file.size / (1024 * 1024)

        st.success(f"‚úÖ File uploaded: **{uploaded_file.name}** ({file_size_mb:.2f} MB)")

        # Check file size
        if file_size_mb > Config.MAX_FILE_SIZE_MB:
            st.error(f"‚ùå File too large: {file_size_mb:.1f}MB (max: {Config.MAX_FILE_SIZE_MB}MB)")
            return

        # Load data
        try:
            with st.spinner("üìñ Reading file..."):
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)

            st.success(f"‚úÖ Loaded {len(df):,} rows and {len(df.columns)} columns")

            # Validate data
            validate_input_dataframe(df)

            # Show preview
            with st.expander("üëÄ Data Preview (First 5 rows)", expanded=False):
                st.dataframe(df.head(), use_container_width=True)

            # Run analysis button
            st.markdown("---")

            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                run_button = st.button("üöÄ Run Analysis", type="primary", use_container_width=True)

            if run_button:
                # Create output directory
                os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

                # Progress tracking
                progress_bar = st.progress(0)
                status_text = st.empty()

                # Run pipeline
                try:
                    result_df = run_pipeline(df, progress_bar, status_text)

                    # Generate timestamp
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                    # Save as Parquet
                    parquet_filename = f"sentiment_output_{timestamp}.parquet"
                    parquet_path = os.path.join(Config.OUTPUT_DIR, parquet_filename)
                    result_df.to_parquet(parquet_path, index=False, compression='snappy')

                    # Also save as CSV
                    csv_filename = f"sentiment_output_{timestamp}.csv"
                    csv_path = os.path.join(Config.OUTPUT_DIR, csv_filename)
                    result_df.to_csv(csv_path, index=False)

                    st.success("üéâ Analysis Complete!")

                    # Display statistics
                    st.markdown("---")
                    st.subheader("üìä Results Summary")

                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("Total Rows", f"{len(result_df):,}")

                    with col2:
                        valid_count = (result_df['Category'] != '').sum()
                        st.metric("Valid Classifications", f"{valid_count:,}")

                    with col3:
                        translated = (result_df['Translated_Text'] != '').sum()
                        st.metric("Translations", f"{translated:,}",
                                 delta=f"{translated/len(result_df)*100:.1f}%")

                    with col4:
                        file_size_kb = os.path.getsize(parquet_path) / 1024
                        st.metric("File Size", f"{file_size_kb:.1f} KB")

                    # Category distribution
                    st.markdown("---")
                    st.subheader("üìà Category Distribution")

                    category_counts = result_df[result_df['Category'] != '']['Category'].value_counts()

                    col1, col2 = st.columns([2, 1])

                    with col1:
                        st.bar_chart(category_counts)

                    with col2:
                        st.dataframe(
                            pd.DataFrame({
                                'Category': category_counts.index,
                                'Count': category_counts.values,
                                'Percentage': (category_counts.values / len(result_df) * 100).round(1)
                            }).reset_index(drop=True),
                            use_container_width=True
                        )

                    # Sentiment distribution
                    st.markdown("---")
                    st.subheader("üí≠ Sentiment Distribution")

                    sentiment_counts = result_df[result_df['Sentiment'] != '']['Sentiment'].value_counts()

                    col1, col2 = st.columns([2, 1])

                    with col1:
                        st.bar_chart(sentiment_counts)

                    with col2:
                        st.dataframe(
                            pd.DataFrame({
                                'Sentiment': sentiment_counts.index,
                                'Count': sentiment_counts.values,
                                'Percentage': (sentiment_counts.values / len(result_df) * 100).round(1)
                            }).reset_index(drop=True),
                            use_container_width=True
                        )

                    # Translation statistics
                    if Config.ENABLE_TRANSLATION:
                        st.markdown("---")
                        st.subheader("üåç Translation Statistics")

                        col1, col2 = st.columns(2)

                        with col1:
                            english_count = len(result_df) - translated
                            st.metric("English Texts", f"{english_count:,}",
                                     delta=f"{english_count/len(result_df)*100:.1f}%")

                        with col2:
                            st.metric("Foreign Language Texts", f"{translated:,}",
                                     delta=f"{translated/len(result_df)*100:.1f}%")

                    # Results preview
                    st.markdown("---")
                    st.subheader(f"üîç Results Preview (First {preview_rows} rows)")
                    st.dataframe(result_df.head(preview_rows), use_container_width=True)

                    # Download section
                    st.markdown("---")
                    st.subheader("üì• Download Results")

                    col1, col2 = st.columns(2)

                    # Parquet download
                    with col1:
                        with open(parquet_path, 'rb') as f:
                            st.download_button(
                                label="üì¶ Download Parquet File",
                                data=f,
                                file_name=parquet_filename,
                                mime="application/octet-stream",
                                use_container_width=True,
                                help="Recommended: Smaller file size, faster loading"
                            )

                    # CSV download
                    with col2:
                        csv_data = result_df.to_csv(index=False)
                        st.download_button(
                            label="üìÑ Download CSV File",
                            data=csv_data,
                            file_name=csv_filename,
                            mime="text/csv",
                            use_container_width=True,
                            help="Alternative: Compatible with Excel"
                        )

                    # File info
                    parquet_size = os.path.getsize(parquet_path) / 1024
                    csv_size = len(csv_data.encode('utf-8')) / 1024
                    compression_ratio = (1 - parquet_size / csv_size) * 100

                    st.info(f"üí° **Parquet is {compression_ratio:.1f}% smaller** ({parquet_size:.1f}KB vs {csv_size:.1f}KB)")

                except Exception as e:
                    st.error(f"‚ùå Error during processing: {str(e)}")
                    st.exception(e)

        except ValueError as e:
            st.error(str(e))
        except Exception as e:
            st.error(f"‚ùå Error loading file: {str(e)}")
            st.exception(e)


# ============================================================
# üöÄ Application Entry Point
# ============================================================

if __name__ == "__main__":
    main()